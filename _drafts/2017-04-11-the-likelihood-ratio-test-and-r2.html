---
layout: post
title: Connection Between The Likelihood Ratio Test and The $R^2$
date: '2017-04-11T19:53:00.001-07:00'
author: Robert Speare
tags: 
modified_time: '2017-04-12T17:37:27.878-07:00'
blogger_id: tag:blogger.com,1999:blog-3410852316732630293.post-2305231796572003719
blogger_orig_url: http://rspeare.blogspot.com/2017/04/the-likelihood-ratio-test-and-r2.html
---

<div dir="ltr" style="text-align: left;" trbidi="on">Recently, I've been working on a regression problem, and heard a lot of talk about $R^2$. Since it's not a measure familiar to me, I typically haven't trusted it as an indicator of performance, but realized today an important connection with the Likelihood Ratio test -- at least for Gaussian loss problems. (There are more technical details in <a href="http://rspeare.blogspot.com/2015/09/equivalence-of-shannon-entropy-and.html">this</a>&nbsp;and <a href="http://rspeare.blogspot.com/2015/08/the-fisher-matrix-and-volume-collapse_31.html">this</a> post on volume collapse when comparing models.)<br /><br />Let's pretend we have two models codified by $\theta_1, \theta_2$, each of which the likelihood and posterior functions look as follows, given data $X$:<br /><br />\begin{eqnarray}<br />\mathcal{L}(X \vert \theta_1), \mathcal{L}(X \vert \theta_2)<br />\end{eqnarray}<br /><br />If we want to compare the models against each other, to see which is a better fit, we can construct the ratio:<br /><br />\begin{eqnarray}<br />\Lambda &amp;=&amp; \frac{\mathcal{L}(X \vert \theta_1)}{ \mathcal{L}(X \vert \theta_2)}<br />\end{eqnarray}<br /><br />For a Gaussian regression problem, we have:<br /><br />\begin{eqnarray}<br />\mathcal{L}(X \vert \theta_1) &amp;=&amp; \frac{1}{(2\pi)^{D_1/2} \mathrm{det}(\Sigma^{1/2})} e^{-\left(X\cdot \beta_1 - Y \right)\Sigma^{-1} \left(X\cdot \beta_1 - Y \right)/2}<br />\end{eqnarray}<br /><br />Where $D_1$ is the number of regressors $\vec{x}$ in our model and $\Sigma$ is the covariance matrix of the data. Taking the log of $\Lambda$ above we have:<br /><br />\begin{eqnarray}<br />-\log(\Lambda) &amp;=&amp; \frac{D_1-D_2}{2}\log(2\pi\ \mathrm{det}\vert \Sigma\vert^{1/2} ) + \left(X\cdot \beta_2 - Y \right)\Sigma^{-1} \left(X\cdot \beta_2 - Y \right)/2 -\\<br />&amp;&amp; \left(X\cdot \beta_1 - Y \right)\Sigma^{-1} \left(X\cdot \beta_1 - Y \right)/2<br />\end{eqnarray}<br /><br />Now, if we assume that the variance on the data is constant, $\Sigma = \sigma^2$ and not in fact a full covariance matrix, we get a very simple expression for our likelihood ratio:<br /><br />\begin{eqnarray}<br />\log(\Lambda) &amp;=&amp; \frac{(D_2-D_1)}{2}\log(2\pi \sigma^2) + \frac{\mathrm(RSS)_2-\mathrm(RSS)_1}{2\sigma^2}\\<br />\mathrm(RSS) &amp;=&amp; \sum_{m,n} (\vec{\beta}\cdot \vec{x}_n - y_n) \Sigma_{mn}^{-1} (\vec{\beta}\cdot \vec{x}_m - y_m)<br />\end{eqnarray}<br /><br />Inspecting this expression, we see that it will be large when model $\theta_1$ drastically reduces prediction error -- or residual sum of squares, $\mathrm{RSS}$ -- from model $\theta_2$; however, if we do that by adding more regressors -- thereby increasing the degrees of freedom from $D_2 \to D_1$, our expression becomes smaller. So we have a push and pull between model fit and model complexity.<br /><br /><br />--------------------------------------------------------------------------------------------------------------------------<br /><br /><br />Since we are comparing two residual sum of squared gaussian random variables -- each of which having $N-D_1$ and $N-D_2$ degrees of freedom -- we know that their difference will be distributed as a $\chi^2$ distribution with $D_1 - D_2$ degrees of freedom. This just comes from:<br /><br />\begin{eqnarray}<br />x &amp;\sim &amp; \mathcal{N}(0, 1) \\<br />x^2 &amp; \sim &amp; \chi^2_1 \\<br />\mathrm{RSS} \approx \sum_{n=1}^{N} x_n^2 &amp; \sim &amp; \chi^2_N \\<br />\mathrm(RSS)_2-\mathrm(RSS)_1 \approx \chi^2_M - \chi^2_N &amp; \sim&amp; \chi^2_{M-N}<br />\end{eqnarray}<br /><br />So let's make a small transformation to our likelihood ratio, scaling things so that the result will be drawn from a unit chi-squared distribution:<br /><br />\begin{eqnarray}<br />\frac{2}{D_1-D_2}\log(\Lambda) &amp;=&amp; \log(2\pi \sigma^2) + \frac{\mathrm(RSS)_2-\mathrm(RSS)_1}{\sigma^2 (D_1 - D_2)} &nbsp;\sim \chi_1^2 + \mathrm{const.}<br />\end{eqnarray}<br /><br />This is fantastic, because now we can quote p-values on the log $\Lambda$. We know that if we reduce variance in the second model by a sufficient amount per increased degree of freedom ($D_1 - D_2$), the increase in complexity is ``worth it'', and therefore given a low p-value.<br /><br />--------------------------------------------------------------------------------------------------------------------------<br /><br />I'll take one moment aside to note that, even if we're not working in the Gaussian regression space, a few things still hold asymptotically. This is because our log likelihood has a nice ``sum'' form for each and every data point:<br /><br />\begin{eqnarray}<br />\mathcal{L}(X \vert \theta) &amp;=&amp; \prod_{n=1}^N \mathcal{L}(x_n \vert \theta) \\<br />\log (\mathcal{L}(X \vert \theta) ) &amp;=&amp; \sum_{n=1}^N \log ( \mathcal{L}(x_n \vert \theta) )<br />\end{eqnarray}<br /><br />As $N \to \infty$ this sum converges in distribution to a Gaussian, even if we're dealing with a poisson loss, a binomial loss, or what have you because of the central limit theorem. And then we're in the domain of Fisher, and the Fisher information matrix $F_{ij}$ mentioned <a href="http://rspeare.blogspot.com/2015/08/">here</a>. For Gaussian regression problems $\log(\Lambda)$ is a ratio of two $\chi^2$ random variables and so is distributed under $F$ -- in the more general case, if we have a difference of two Gaussian random variables, as described above, $\log(\Lambda)$ would be asymptotically Gaussian as well. <br /><br />--------------------------------------------------------------------------------------------------------------------------<br /><br />When one doesn't actually know the variance of our target, $\sigma^2$, we can estimate it by writing:<br /><br />\begin{eqnarray}<br />\hat{\sigma}^2 &amp;=&amp; (\mathrm{RSS})_1/N<br />\end{eqnarray}<br /><br />Taking the residual sum of squares on our most complex \&amp; accurate model. (The reason for this will become clear in a moment.) Now the log of the likelihood ratio becomes:<br /><br />\begin{eqnarray}<br />\frac{2}{N}\log(\Lambda)&amp;=&amp; (D_1-D_2) \log(2\pi \sigma^2) +\mathrm(RSS)_2/\mathrm(RSS)_1 - 1 \sim \chi_{D_1-D_2}^2 + \mathrm{const.}<br />\end{eqnarray}<br /><br />And this number will be strictly greater than 1 <b>if</b> we reduce prediction error when moving from model 2 to model 1.<br /><br />--------------------------------------------------------------------------------------------------------------------------<br /><div><br />And why do we care about this? well, if you look at the definition of $R^2$,&nbsp;</div><div><br />\begin{eqnarray}<br />R^2 &amp;=&amp; 1- \frac{(\mathrm{RSS})_1}{(\mathrm{RSS})_2}<br />\end{eqnarray}</div><div><br /></div><div>we see that this is precisely the ratio of the log likelihoods:</div><div><div><br class="Apple-interchange-newline" />\begin{eqnarray}<br />R^2 &amp;=&amp; 1-e^{-\frac{2}{N} \log(\Lambda ) - \mathrm{const.} }<br />\end{eqnarray}</div><div><br /></div><div>and so we can now assign p-values to this "generalized" $R^2$ value -- very useful for feature selection.&nbsp;</div></div><br /></div>