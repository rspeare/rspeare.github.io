---
layout: post
title: Information Theory, "Adiabatic" in QM and Statmech, and the expanding 1D box
date: '2015-02-08T17:21:00.000-08:00'
author: Robert Speare
tags: 
modified_time: '2015-02-09T09:31:58.271-08:00'
blogger_id: tag:blogger.com,1999:blog-3410852316732630293.post-2076120663382654446
blogger_orig_url: http://rspeare.blogspot.com/2015/02/information-theory-adiabatic-in-qm-and.html
---

<div dir="ltr" style="text-align: left;" trbidi="on">So, the use of the word "Adiabatic" in Quantum mechanics normally has to do with slow processes, or slow perturbations to an initial hamiltonian of the form<br /><br />\begin{eqnarray}<br />H &amp;=&amp; H_0+V(t)<br />\end{eqnarray}<br /><br />Where $H_0$ is our unperturbed Hamiltonian with the eigenkets:<br /><br />\begin{eqnarray}<br />H_0 \vert n \rangle &amp;=&amp; E_n \vert n \rangle<br />\end{eqnarray}<br /><br />One can solve this problem using time-dependent perturbation theory, and for potentials that are turned on very, very slowly,<br /><br />\begin{eqnarray}<br />V(t) &amp;=&amp; Ve^{\eta t} \\<br />\eta &lt;&lt; 1<br />\end{eqnarray}<br /><br />it becomes clear that initial eigenkets $\vert n \rangle$ do not mix. Or, in other words, the time evolution operator is essentially the identity matrix in the $\vert n \rangle $ representation.<br /><br />Now this is interesting, because Adiabatic means no change in the composition of our wavefunction, or for many particles, no change in the statistical ``mixture'' of our system. The best way to represent a mixture in Quantum mechanics is through the density matrix, $\rho$, which is a hermitian matrix of trace one:<br /><br />\begin{eqnarray}<br />\rho &amp;=&amp;\sum \vert c_n\vert^2 \vert n \rangle \langle n \vert<br />\end{eqnarray}<br /><br />Where I've written this in a diagonal basis. Since the elements are positive definite, we can bound them between $ 0 &lt; \vert c_n \vert^2 &lt; 1$, and in fact call them ``weights'' or ``probabilities'', of different eigenkets in our mixture. In my mind I associate diagonal elements of $\rho $ with the probability of being in certain eigenstates of the hamiltonian. One could treat the diagonal elements, in the limit as this matrix has $N^2$ elements and $N \to \infty$ as a pure probability density function, indexed by $n$.<br /><br />So why do we care? Well, in Statistical mechanics, the density matrix turns out to be the best way to describe entropy:<br /><br />\begin{eqnarray}<br />\sigma &amp;=&amp; -\mathrm{Tr}\left[\rho \log(\rho)\right]<br />\end{eqnarray}<br /><br />And since the Trace is independent of basis, we can write this as:<br /><br />\begin{eqnarray}<br />\sigma &amp;=&amp; -\mathrm{Tr}\left[\rho \log(\rho)\right] \\<br />&amp;=&amp; \sum_i -\rho_{ii} \log (\rho_{ii})<br />\end{eqnarray}<br /><br />Which, for you information theory people, is just Shannon's definition of entropy, if you replace $\rho_{ii} \to P(x)$.<br /><br />Now what justifies this formulation of entropy? Normally we talk about entropy as the logarithm of total number of accessible microstates, associated with some fixed macro states. If we are to adopt a simple model of placing $N_1$ particles in state 1, $N_2$ particles in states 2, etc. all the way up to $N_n$ in state $n$; placing $N=N_1+N_2+\dots N_n$ particles into our system in all, we can write the number of microstates in a clever combinatorical way:<br /><br />\begin{eqnarray}<br />\Omega &amp;=&amp; \frac{N!}{N_1!N_2!\cdots N_n!}<br />\end{eqnarray}<br /><br />Taking the logarithm and applying stirling's approximation to this expression, it is a fairly simple exercise to show that the entropy is:<br /><br />\begin{eqnarray}<br />\log \Omega &amp;=&amp; -\sum_{i=1}^n \frac{N_i}{N}\log (\frac{N_i}{N})<br />\end{eqnarray}<br /><br />But these fractions $N_i/N$ are just the probabilities of placing our ``particles'' into the $i^\mathrm{th}$ state! So this is exactly equivalent to shannon's entropy -- in discrete form -- and to our definition of entropy from the viewpoint of the density matrix.<br /><br />----------------------------------------------------------<br /><br />Now, why am I making such a big fuss about the $\sigma$ in terms of $\rho$? Well, because, there's a typical problem in quantum mechanics -- and thermo -- which is placing $N$ particles -- let's make them bosons -- in an infinite square well. The particles can't escape, and we find that the energy is quantized for each one as:<br /><br />\begin{eqnarray}<br />V(x) &amp;=&amp; 0 \ \ \mathrm{for} \ \ 0&lt;x&lt;L\\<br />&amp;=&amp; \infty \ \ \mathrm{otherwise} \\<br />E_n &amp;=&amp; \frac{\hbar^2 \pi^2 n^2}{2mL^2}<br />\end{eqnarray}<br /><br />And one might ask, what is the distribution of eigenkets in such a system, if I place $N$ bosons in this box? What happens if I ``adiabatically'' in the sense of quantum mechanics, expand the boxes width, so at the end of my perturbation I have sent $L \to 2L$? Or, in the obverse sense, what if I make such a change <b>extremely rapidly, requiring that the box is thermally insulated and can absorb no heat </b>from any surrounding system? The latter sentence as to do with ``adiabatic'' in the statistical mechanics sense. So what do these have to do with each other?<br /><br />-------------------------------------------------------<br /><br />It is first important to note that, in thermal equilibrium -- when we typically maximize entropy -- the form of the density matrix is:<br /><br />\begin{eqnarray}<br />\rho_nn &amp;=&amp; e^{-\beta E_n}/Z \\<br />Z &amp;=&amp; \sum_n e^{-\beta E_n}<br />\end{eqnarray}<br /><br />This can be shown by maximizing $\sigma$ given the constraint that total energy equals some specified value, $\langle E \rangle$, and the density matrix is properly normalized. $\beta$ in this exercise acts as a simple lagrange multiplier. But of course, we know this as the boltzman distribution, and association $\beta$ with one over Temperature times Boltzman, and inverse energy parameter.<br /><br />For our free particle in a infinite square well, we expect<br /><br />\begin{eqnarray}<br />\rho_{nn} &amp;=&amp; e^{-\beta \frac{\hbar^2 \pi^2 n^2}{2mL^2}}/Z \\<br />Z &amp;=&amp; \left(e^{\frac{\hbar^2 \pi^2 \beta}{2mL^2}} -1\right)^{-1}<br />\end{eqnarray}<br /><br />If we calculate the expectation value of energy in such a system, we get:<br /><br />\begin{eqnarray}<br />\langle E \rangle &amp;=&amp; -\frac{\partial \log Z}{\partial \beta}=\frac{\frac{\hbar^2\pi^2}{2mL^2}}{&nbsp;\left(1-e^{\frac{\hbar^2 \pi^2 \beta}{2mL^2}} \right)}<br />\end{eqnarray}<br /><br />In the classical limit we set $\hbar \to 0$, and so this yields $\langle E \rangle = \beta^{-1}$. Which, is off by a factor of a half, but I won't fret about it at the moment. This simply corroborates the equipartition theorem, which states that in the high temperature -- or classical -- limit, we expect the exponent above to be small, and for each degree of freedom to contribute $k_B T$ to the energy.<br /><br />Now, doing out the math on entropy, for our free particle confined to a box, I get:<br /><br />\begin{eqnarray}<br />\log \rho_{nn} &amp;=&amp; -\beta \frac{\hbar^2 \pi^2 n^2}{2mL^2}-\log{Z}\\<br />\sigma &amp;=&amp; \beta \langle E \rangle + \log (Z)<br />\end{eqnarray}<br /><br />Which is a very interesting -- and, I checked it, correct -- result. We can get some intuition for this formula by looking once again at the classical limit, and setting<br /><br />\begin{eqnarray}<br />Z &amp;=&amp; &nbsp;\left(e^{\frac{\hbar^2 \pi^2 \beta}{2mL^2}} -1\right)^{-1}\\<br />&amp;\approx &amp; &nbsp;\frac{2mL^2}{\beta \hbar^2 \pi^2}<br />\end{eqnarray}<br /><br />Such that our entropy becomes<br /><br />\begin{eqnarray}<br />\sigma &amp;=&amp; \log \left( \frac{2mL^2}{\beta \hbar^2 \pi^2} \right) + \beta \beta^{-1} <br />\end{eqnarray}<br /><br />So we see that, for expanding our box ``quickly'', $L\to 2L$, when all settles down, we expect such a system to have four times as many accessible microstates. This makes sense from a phase space or quasi-classical point of view because the resolution of both the $x$ and $p$ space goes up by a factor of 2.<br /><br />A more interesting question is, after we make such a sudden change, what is the density matrix<br />$\rho$ for our system? We do not expect it to be thermal, but time will tell. I will have to investigate things in mathematica, using a change of basis between $L$ and $2L$ infiinite well eigenstates.<br /><br /><br /><br /><br /></div>