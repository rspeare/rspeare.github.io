<!DOCTYPE html>
<html lang="en-us">

  <head>
  <meta charset="UTF-8">
  <title>rspeare.github.io</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/css/cayman.css">
</head>


  <body>
    <section class="page-header">
  <h1 <a href="/" class="project-name">rspeare.github.io </a> </h1> 
  <h2 class="project-tagline"></h2>
  <a href="/" class="btn">Blog</a>  

  <a class="btn" <a href="https://github.com/rspeare"><span class="icon icon--github"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">rspeare</span></a>
 </a>


  <a class="btn" <a href="https://twitter.com/waiting4spark"><span class="icon icon--twitter"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">waiting4spark</span></a>
 </a>

<a href="/about/" class="btn">About</a>  
</section>


    <section class="main-content">
      
      <style TYPE="text/css">
code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
</style>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
    }
});
MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<article class="post" itemscope itemtype="http://schema.org/BlogPosting">

<h2>Multi-Class Logistic Regression as Maximum (conditional) Entropy</h2>
<p class="meta">27 Jul 2015</p>

<div dir="ltr" style="text-align: left;" trbidi="on">Recently, I've been 
thinking about Logistic regression -- i.e. Machine Learning classification and 
the activation of neurons in neural networks -- and how it can be framed in 
terms of Statistical Mechanics. If you look at the sigmoid function, you'll 
notice something a bit suspect -- or at least I did: it looks peculiarly like 
a Boltzman distribution, or some properly normalized member of the exponential 
family: 

\begin{eqnarray} 
\mathrm{sigm}(x)&amp;=&amp; \frac{1}{1+e^{-x}} \\ 
\mathrm{sigm}(\beta x)&amp;=&amp; \frac{1}{1+e^{-\beta x}} 
\end{eqnarray} 

If we scale this function by $\beta$, this looks a lot like Boltzman. And, if 
we use the standard framework for logistic regression, where our exponential 
argument is the dot product between a vector of parameters $\vec{\theta}$ and 
a vector of input data, $\vec{x}$, we have: 

\begin{eqnarray} 
P(y=1 \vert \vec{x},\vec{\theta}) &amp;=&amp;  \frac{1}{1+e^{-\vec{\theta} 
\cdot \vec{x}}} 
\end{eqnarray} 

That is, the probability that our feature factor $\vec{x}$ belongs to class 
$y=1$ is given by the sigmoid function, with a ``linear regression'' as its 
argument. Could we derive this sigmoid function by different means? Where does 
it come from? 

Recall that the Boltzman distribution can be derived from maximum entropy. 
Shannon proved that the entropy of a probability distribution is equal to the 
following strange quantity: 

\begin{eqnarray} 
S[p] &amp;=&amp; \sum_i -p(x_i) \log \left(p(x_i)\right) 
\end{eqnarray} 

or, in the continuous PDF case: 

\begin{eqnarray} 
S[p] &amp;=&amp; \int -p(x) \log \left( p(x) \right) dx 
\end{eqnarray} 

another way to write this is 

\begin{eqnarray} 
S &amp;=&amp; \langle -\log\left(p(x) \right) \rangle 
\end{eqnarray} 

Maximizing this entropy, subject only to normalization, involves setting the 
gradient of the following function to zero: 

\begin{eqnarray} 
 G&amp;=&amp; S[p] + \lambda \left( \sum_i p(x_i) -1 \right)\\ 
 \frac{\partial G}{\partial p(x_i)} &amp;=&amp; 0 \ \forall i 
\end{eqnarray} 

If we have a discrete distribution over $N$ total points $\lbrace x_i 
\rbrace_{i=1}^N$, we get the \textbf{**uniform distribution**}. 

\begin{eqnarray} 
p(x_i) &amp;=&amp; \frac{1}{N} 
\end{eqnarray} 

But, once we start specifying certain properties of this distribution, like, 
say it's moments, we will change things substantially. Constraining on the 
mean gives us the exponential distribution: 

\begin{eqnarray} 
G[p]&amp;=&amp; S[p] + \lambda \left( \int p(x) -1 \right)+ \beta \left( \int 
p(x) x - \tau \right)\\ 
 \frac{\delta G}{\delta p(x_i)} &amp;=&amp; 0 \ \forall i \\ 
 \Rightarrow p(x_i) &amp;=&amp; \tau e^{-\tau x_i } 
\end{eqnarray} 

Where now instead of a gradient we've taken a \textbf{**functional 
derivative**}, of the functional $G$. Our result is the exponential 
distribution, which is the \textbf{**maximally entropic**} probability 
distribution with its first moment constrained. (You may not be surprised to 
learn that the Gaussian is the maximally entropic PDF with its first and 
second moment constrained!) 

So maximizing entropy subject to certain constraints places heavy, 
deterministic restrictions on the form of our PDF's. We are essentially 
designing things to be as ``uninformative'' or least biased as possible. As 
Jaynes once said, `our thinking Robot is non-ideological: it takes in only the 
information it is given and nothing more'. 

When we do classification problems, we are really maximizing the entropy of a 
probability distribution on class $y$, not on the feature vector $\vec{x}$, 
and this conditional entropy has a specific form: 

\begin{eqnarray} 
S[p(y \vert \vec{x})] &amp;=&amp; -\sum_i p(\vec{x}) p(y \vert \vec{x})\log 
\left( p(y \vert \vec{x}) \right) 
\end{eqnarray} 

Ok. So now what are our constraints? We would like to define some function of 
the feature vector $\vec{x}$ that is useful, and require our probability 
density on class $y$, $p(y \vert \vec{x}$, to exhibit the \textbf{**same 
ensemble averages**}: 

\begin{eqnarray} 
\sum_{\vec{x},y} p(\vec{x}) p(y \vert \vec{x}) f_y(\vec{x}) &amp;=&amp; 
\langle f_y(\vec{x}) \rangle 
\end{eqnarray} 

Where on the left we are summing over only $\vec{x}$ that are in the training 
set. (This is the role of $p(\vec{x})$. I have seen this equation called a 
``balance'' equation, but really it is just a constraint on the ensemble 
average -- over class $y$ and feature vector $\vec{x}$ -- of some function 
$f_y(\vec{x})$: just like how we constrain energy with the Boltzman 
distribution: 

\begin{eqnarray} 
\sum_{\vec{x}} P(\vec{x}) \mathcal{H}(\mathbf{x}) &amp;=&amp; \langle E 
\rangle 
\end{eqnarray} 

Putting things into our big expression for entropy, we have: 

\begin{eqnarray*} 
G[p] &amp;=&amp; -\sum_i p(\vec{x}) p(y \vert \vec{x})\log \left( p(y \vert 
\vec{x}) \right) + \lambda \left( \sum_y p(y \vert \vec{x}) -1 \right)+ \beta 
\left(\sum_{\vec{x},y} p(\vec{x}) p(y \vert \vec{x}) f_y(\vec{x}) - \langle 
f_y(\vec{x}) \rangle \right) 
\end{eqnarray*} 

So $\lambda$ is the lagrange multiplier that asserts proper normalization on 
the PDF over class, not feature vector $\vec{x}$, and $\beta$ is the lagrange 
multiplier that asserts proper expectation value of the function 
$f_y(\mathbf{x})$ over both class and the training set. We could extend this 
and have \textbf{**multiple**} feature functions to be constrained on, with 
multiple lagrange multpliers by just making them both vector valued: 

\begin{eqnarray*} 
G[p] &amp;=&amp; -\sum_i p(\vec{x}) p(y \vert \vec{x})\log \left( p(y \vert 
\vec{x}) \right) + \lambda \left( \sum_y p(y \vert \vec{x}) -1 \right)+ 
\vec{\beta} \cdot \left(\sum_{\vec{x},y} p(\vec{x}) p(y \vert \vec{x}) 
\vec{f}_y(\vec{x}) - \langle \vec{f}_y(\vec{x}) \rangle \right) 
\end{eqnarray*} 

Setting the gradient -- or in the continuous case, functional derivative -- of 
this expression to zero gives: 

\begin{eqnarray} 
P(y \vert \vec{x}) &amp;=&amp; Z^{-1} e^{\vec{\beta} \cdot 
\vec{f_y}(\vec{x})}\\ 
Z &amp;=&amp; \sum_c e^{\vec{\beta} \cdot \vec{f_y}(\vec{x})} 
\end{eqnarray} 

This expression already has multi-class classification built in, but let's see 
what it looks like for simpler logistic regression, when $y=0,1$. What is a 
good choice for the feature function $\vec{f_y}(\vec{x})$ to get our sigmoid 
function back? Since logistic regression looks a lot like a 
``sigmoid-wrapped'' linear regression in textbooks, it might be a good idea to 
define: 

\begin{eqnarray} 
\vec{f_y}(\vec{x}) &amp;=&amp; \vec{x} \ \ \mathrm{if} \ \ y^\prime=y 
\end{eqnarray} 

Then we have: 

\begin{eqnarray} 
P(y=1 \vert \vec{x}) &amp;=&amp; \frac{ e^{\vec{\beta_1} \cdot 
\vec{x}}}{e^{\vec{\beta_0} \cdot \vec{x}}+e^{\vec{\beta_1} \cdot \vec{x}}} 
\end{eqnarray} 

Simplifying things we get: 

\begin{eqnarray} 
P(y=1 \vert \vec{x}) &amp;=&amp; \frac{ 1}{1+e^{-(\vec{\beta_1}-\vec{\beta_0}) 
\cdot \vec{x}}} 
\end{eqnarray} 

The expression above is precisely logistic regression, with the parameters 
$\beta_0$ the ``anti'' linear-coefficients and $beta_1$ the non-anti linear 
coefficients. We combine them both into the normal $\vec{\theta}$ and write 

\begin{eqnarray} 
P(y=1 \vert \vec{x}) &amp;=&amp; \frac{ 1}{1+e^{-\vec{\theta} \cdot \vec{x}}} 
\end{eqnarray} 

So what does this mean? It means that $\beta_1$ is constraining the ensemble 
average of the $i^\mathrm{th}$ component of the feature vector $\vec{x}$, 
given that class is $y=1$;  And conversely for $\beta_0$, we are constraining 
the $i^\mathrm{th}$ component of the feature vector when the class is $y=0$. 
These feature functions can be called marginal counts, or marginal 
probabilities on $\vec{x}$. 

Now in the last post I talked about how the vector $\theta$ gives the 
direction of \textbf{**fastest increase**} for the logistic function, or the 
direction of ``fastest decision'' -- both for yes and no -- from the decision 
boundary. We can frame this in an even more physical way using Lagrange 
multipliers, because what $\vec{\beta}$ really represents in feature space is 
a \textbf{**constraint force**}, as seen in Lagrangian mechanics. Just like 
when we are minimizing the action -- a functional -- subject to some 
constraint: 

\begin{eqnarray} 
S &amp;=&amp; \int \left[ \mathcal{L(q,\dot{q})}+ \lambda \left(q_x^2+q_y^2 - 
R^2\right)  \right] dt 
\end{eqnarray} 

$\lambda$ above gives the ``tension'' force $\lambda=T$ for a massive 
particle, constrained to move on a circle of radius $R$. We are using the same 
tactic with $\beta$ to ``push'' our PDF into the proper shape. Stronger 
components -- i.e. larger scalar values within $\vec{\beta}$ -- correspond to 
stronger entropic forces on the PDF, and stronger correlation. 

It is easy to extend the expressions above to multiple classes $y=0,1,2 
\dots$, and important to note that the activation of most all neurons in 
neural networks use logistic regression: it's all maximum entropy learning. 
E.T. Jaynes would be very proud. 
</div>




      <footer class="site-footer">
  <span class="site-footer-owner"><a href="http://localhost:4000">rspeare.github.io</a> is maintained by <a href="">rspeare</a>.</span>
  <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>

  <a href="https://www.linkedin.com/in/rob-speare-aaa6834a">
    <span class="icon  icon--linkedin">
      <svg viewBox="0 0 10000 10000" >
        <path fill="#828282" d="M150.65,100.682c0,27.992-22.508,50.683-50.273,50.683c-27.765,0-50.273-22.691-50.273-50.683
        C50.104,72.691,72.612,50,100.377,50C128.143,50,150.65,72.691,150.65,100.682z M143.294,187.333H58.277V462h85.017V187.333z
        M279.195,187.333h-81.541V462h81.541c0,0,0-101.877,0-144.181c0-38.624,17.779-61.615,51.807-61.615
        c31.268,0,46.289,22.071,46.289,61.615c0,39.545,0,144.181,0,144.181h84.605c0,0,0-100.344,0-173.915
        s-41.689-109.131-99.934-109.131s-82.768,45.369-82.768,45.369V187.333z"/>
      </svg>
    </span>

    <span class="username"></span>
  </a>

</footer>
 

    </section>

  </body>
</html>
