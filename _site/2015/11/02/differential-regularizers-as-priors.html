<!DOCTYPE html>
<html lang="en-us">

  <head>
  <meta charset="UTF-8">
  <title>rspeare.github.io</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/css/cayman.css">
  
</head>


  <body>
    <section class="page-header">
  <h1 <a href="/" class="project-name">rspeare.github.io </a> </h1> 
  <h2 class="project-tagline"></h2>
  <a href="/" class="btn">Blog</a>  

  <a class="btn" <a href="https://github.com/rspeare"><span class="icon icon--github"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">rspeare</span></a>
 </a>


  <a class="btn" <a href="https://twitter.com/waiting4spark"><span class="icon icon--twitter"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">waiting4spark</span></a>
 </a>

<a href="/about/" class="btn">About</a>  
</section>


    <section class="main-content">
      
      <style TYPE="text/css">
code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
</style>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
    }
});
MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<article class="post" itemscope itemtype="http://schema.org/BlogPosting">

<h2>Differential Regularizers as Priors over Functions $f(x)$: Gaussian Process Kernels as Green's Functions</h2>
<p class="meta">02 Nov 2015</p>

<div dir="ltr" style="text-align: left;" trbidi="on">Often, when we are 
solving a regression problem, we are given the following functional: 

\begin{eqnarray} 
J[f] &amp;=&amp; l\left[f,y\right]+\Omega \left[f \right] 
\end{eqnarray} 

Where $l$ is some loss functional and $\Omega$ is some regularizer. Given some 
finite dataset, this might look like, in the case of squared loss: 

\begin{eqnarray} 
X &amp;=&amp; \lbrace \mathbf{x}_n \rbrace_{n=1}^N \\ 
J &amp;=&amp; \sum_{n=1}^N \left( f(\mathbf{x}_n)-y_n \right)^2 + \Omega(f) 
\end{eqnarray} 

For a linear basis expansion model, we have the following: 

\begin{eqnarray} 
f(\mathbf{x}) &amp;=&amp; \mathbf{w}\cdot \phi(x) \\ 
J(\mathbf{w}) &amp;=&amp; \sum_{n=1}^N \left(  \mathbf{w}\cdot \phi(x_n) -y_n 
\right)^2 + \lambda \vert \mathbf{w} \vert^2 
\end{eqnarray} 

where $\lambda \vert \mathbf{w} \vert^2$ plays the role of a prior over 
functions. The cost function in this example is proportional to the negative 
log prior, what we have is essentially: 

\begin{eqnarray} 
-\log P(\mathbf{w} \vert X, \mathbf{y}, \phi) &amp; \sim &amp; J(\mathbf{w}) 
\\ 
-\log \mathcal{L}(X, \mathbf{y} \vert \mathbf{w} \phi) &amp; \sim &amp; 
\sum_{n=1}^N \left(  \mathbf{w}\cdot \phi(x_n) -y_n \right)^2 \\ 
-\log P(\mathbf{w} \vert \phi) &amp; \sim &amp;  \Omega(f) = \lambda \vert 
\mathbf{w} \vert^2 
\end{eqnarray} 

(So that's a Gaussian likelihood and a Gaussian prior). Minimizing the cost 
with respect to $\mathbf{w}$ is same thing as finding the mode of the 
posterior, or Maximum a Posteriori. (MAP). We've already talked about how, for 
such a regularized regression problem, we can right the solution as a linear 
combination of kernels, centered at the data: 

\begin{eqnarray} 
f(\mathbf{x}) &amp;=&amp; \sum_{n=1}^N \alpha_n K(\mathbf{x},\mathbf{x}_n ) 
\end{eqnarray} 

a manifestation of the representer theorem. But one important question to ask, 
given some regularization functional, is, what's the ``best'' Kernel? Let's 
take for example the regularizer: 

\begin{eqnarray} 
J &amp;=&amp; \int dx \left(f(x)-y \right)^2 + \lambda \int dx 
\vert\frac{\partial^2 f(x)}{\partial x^2}\vert^2 
\end{eqnarray} 

This $\Omega$ functional penalizes curvature in our fitting function $f(x)$, 
and we can note that what such a regularizer really is, is a prior over 
functions, since: 

\begin{eqnarray} 
P(f  \vert f(X), \mathbf{y} ) &amp;=&amp;\frac{P(X,y \vert f) P(f)}{P(X, 
\mathbf{y})} \\ 
&amp;=&amp; \frac{\mathrm{exp}\left[-\int dx \delta^D(y-f(X)) \left(f(x)-y 
\right)^2 - \lambda \int dx \vert\frac{\partial^2 f(x)}{\partial x^2}\vert^2 
\right] }{P(X, \mathbf{y})} \\ 
\end{eqnarray} 

We see that the prior on our function is: 

\begin{eqnarray} 
P[f] &amp;\sim &amp; \mathrm{exp}\left(-\lambda \int \vert f^{\prime 
\prime}(x)\vert^2 dx \right) 
\end{eqnarray} 

where $\lambda$ controls the "strength" of our penalty for curvature. To be 
more general, we could have written the prior on our function as a 
superposition of differential operators: 

\begin{eqnarray} 
P[f] &amp;\sim &amp; \mathrm{exp}\left(-\int \sum_{m=1}^\infty a_m 
\frac{\partial^m }{\partial x^m}\vert f(x)\vert^2 dx \right) 
\end{eqnarray} 

If we integrate by parts, we note that this prior functional can be put into 
the form: 

\begin{eqnarray} 
P[f] &amp;\sim &amp; \mathrm{exp}\left(-\int dx dx^\prime f(x) 
K(x,x^\prime)^{-1} f(x^\prime) \right) 
\end{eqnarray} 

Which of course gives us the familiar prior assumptions that $f(x)$ has mean 
and covariance: 

\begin{eqnarray} 
\langle f(x) \rangle &amp;=&amp; 0 \\ 
\mathrm{Var}\left(f(x) \right) &amp;=&amp; K(x,x^\prime) 
\end{eqnarray} 

But, for a given differential regularizer, how do we find the associated 
Kernel? The answer is simple, it's just the Green's function of the operator: 

\begin{eqnarray} 
\hat{L} &amp;=&amp; \sum_m a_m \frac{\partial^m}{\partial x^m}\\ 
\hat{L}K &amp;=&amp; \sum_m a_m \frac{\partial^m}{\partial x^m} K(x,x^\prime) 
= \delta(x-x^\prime) 
\end{eqnarray} 

An easy way to get the green's function -- or in this case Kernel -- is to 
fourier transform. We can re-write our prior in $s$ space: 

\begin{eqnarray} 
f(s) &amp;=&amp; \int dx e^{-isx}f(x) \\ 
P[f] &amp;\sim &amp; \mathrm{exp}\left(-\int ds\sum_{m=1}^\infty a_m 
\vert\mathbf{s}\cdot \mathbf{s}\vert^m\vert f(s)\vert^2 dx \right) 
\end{eqnarray} 

We see now the fourier transform of our inverse kernel is: 

\begin{eqnarray} 
\frac{1}{K(s,s^\prime)} &amp;=&amp; \sum_m a_m (-1)^m \vert \mathbf{s}\cdot 
\mathbf{s} \vert^m \delta^D(\mathbf{s}+\mathbf{s}^\prime) 
\end{eqnarray} 

We see that the kernel is diagonal and in $s$ space and semi-positive 
definite. Which means that we are translationally invariant in $x$ space. We 
have: 

\begin{eqnarray} 
K(x,x^\prime) &amp;=&amp; \int ds ds^\prime e^{isx+is^\prime x^\prime} 
\frac{1}{\sum_m a_m (-1)^m \vert \mathbf{s}\cdot \mathbf{s}^\prime 
\vert^m}\delta^D(\mathbf{s}+\mathbf{s}^\prime)\\ 
K(x-x^\prime) &amp;=&amp; \int ds  e^{is(x-x^\prime)} \frac{1}{\sum_m a_m 
\vert \mathbf{s}\cdot \mathbf{s} \vert^m} 
\end{eqnarray} 

We see that, indeed, our Kernel will be translationally invariant, and when we 
put a prior over functions, what is essentially a Lagrangian in physics: 

\begin{eqnarray} 
\Omega[f] \sim L &amp;=&amp; \int dx f(x) \left(\sum_m a_m 
\frac{\partial^m}{\partial x^m}\right) f(x) 
\end{eqnarray} 

We find that the kernel is just the correlation function -- or, the propagator 
-- for the resulting stochastic process. One example is the massive free 
particle in higher dimensions -- or the free process in higher dimensional 
feature space -- for which we get the Yukawa potential: 

\begin{eqnarray} 
\Omega[f] \sim L &amp;=&amp; \int dx \frac{m^2}{2}f(x)^2+\frac{1}{2}\nabla^2 
f(x) \\ 
K(x-x^\prime) &amp;\sim &amp; \frac{1}{\vert x - x^\prime \vert }e^{-\vert 
x-x^\prime \vert m} 
\end{eqnarray} 

So, the Kernel we use when interpolating some field, or, specifying the prior 
of our Gaussian process, can be viewed as the Green's function to our 
penalizing "regularizer". If we want smooth functions up to order $m=M$, we 
know precisely how to formulate the associated $K(x-x^\prime)$. Note that all 
differential regularizers, such as discussed here will lead to stationary 
kernels and thus stationary processes. 
</div>




      <footer class="site-footer">
  <span class="site-footer-owner"><a href="http://localhost:4000">rspeare.github.io</a> is maintained by <a href="">rspeare</a>.</span>
  <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>

  <a href="https://www.linkedin.com/in/rob-speare-aaa6834a">
    <span class="icon  icon--linkedin">
      <svg viewBox="0 0 10000 10000" >
        <path fill="#828282" d="M150.65,100.682c0,27.992-22.508,50.683-50.273,50.683c-27.765,0-50.273-22.691-50.273-50.683
        C50.104,72.691,72.612,50,100.377,50C128.143,50,150.65,72.691,150.65,100.682z M143.294,187.333H58.277V462h85.017V187.333z
        M279.195,187.333h-81.541V462h81.541c0,0,0-101.877,0-144.181c0-38.624,17.779-61.615,51.807-61.615
        c31.268,0,46.289,22.071,46.289,61.615c0,39.545,0,144.181,0,144.181h84.605c0,0,0-100.344,0-173.915
        s-41.689-109.131-99.934-109.131s-82.768,45.369-82.768,45.369V187.333z"/>
      </svg>
    </span>

    <span class="username"></span>
  </a>

</footer>
 

    </section>

  </body>
</html>
