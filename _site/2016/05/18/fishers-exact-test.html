<!DOCTYPE html>
<html lang="en-us">

  <head>
  <meta charset="UTF-8">
  <title>rspeare.github.io</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/css/cayman.css">
</head>


  <body>
    <section class="page-header">
  <h1 <a href="/" class="project-name">rspeare.github.io </a> </h1> 
  <h2 class="project-tagline"></h2>
  <a href="/" class="btn">Blog</a>  

  <a class="btn" <a href="https://github.com/rspeare"><span class="icon icon--github"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">rspeare</span></a>
 </a>


  <a class="btn" <a href="https://twitter.com/waiting4spark"><span class="icon icon--twitter"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">waiting4spark</span></a>
 </a>

<a href="/about/" class="btn">About</a>  
</section>


    <section class="main-content">
      
      <style TYPE="text/css">
code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
</style>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
    }
});
MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<article class="post" itemscope itemtype="http://schema.org/BlogPosting">

<h2>Fisher's Exact Test, Lift, and P-Values for Feature Exploration</h2>
<p class="meta">18 May 2016</p>

<div dir="ltr" style="text-align: left;" trbidi="on"> 
Let's say you are asked to solve a binary classification problem ($y=0,1$) 
with very few training examples ($N&lt;1000$) and quite a few, possibly 
predictive features ($d&gt;1000$). The standard question of "how the heck do I 
feature select?" becomes very relevant, and in particular, "how the heck do I 
feature select with so few training examples?!?". 

For Categorical features, one of the best ways to test for significance -- 
i.e. a non-null relationship between a feature column and a label column -- is 
Fisher's exact test and Laplace-smoothed lift. 

Fisher's exact test is a combinatoric way of examining a contingency (or 
pivot) table. Let's say we have two columns $x$ and $y$, which both take on, 
in the simplest case, only two values: true and false. If we were to make 
pivot table, we'd have the number of pair-wise events in a 2x2 grid. 

\begin{eqnarray} 
\mathrm{pivot}(x,y) &amp;=&amp; \left( \begin{array}{cc} n_{00} &amp; n_{01} 
\\ 
n_{10} &amp; n_{11} 
\end{array}\right) 
\end{eqnarray} 

Where $n_{11}$ corresponds to the number of times $x$ and $y$ were both True, 
$n_{00}$ corresponds to the number of times $x$ and $y$ were both false, 
$n_{10}$ the number times $x$ was true and $y$ false, etc. (This can be done 
easily in pandas by writing something like  
\textit{pandas.pivot(dataFrame,index=x,columns=y,aggFunc=len,fillna=0}).) 

We can see that the sum of the entries $n_{11}+n_{01}+n_{10}+n_{00}=N$ equals 
the number of training examples, and that the sum over rows or columns equals 
the marginal counts. $n_{11}+n_{10}=R_1$ equals the number times $x$ was true; 
$n_{01}+n_{11}=C_1$ equals the number times $y$ was true; $n_{01}+n_{00}=R_0$ 
equals the number times $x$ was false, etc. 

What fisher proposed is to take this matrix and ask, given the marginal 
counts, $R_0,R_1,C_0,C_1$ -- which, if you think about it, correspond to the 
prior probabilities on $x$ and $y$: $P(x)=\frac{R_1}{N},P(y)=\frac{C_1}{N}$ -- 
how likely is the resulting contingency matrix if $x$ and $y$ are independent? 

The most naive way to answer that question is to take the prior probabilities 
on $x$ and $y$ that's given two us by the data: 

\begin{eqnarray} 
P(x)=p &amp;=&amp; \frac{R_1}{N} \\ 
P(y)=q &amp;=&amp; \frac{C_1}{N} 
\end{eqnarray} 

and quote our good old multinomial: 

\begin{eqnarray} 
P(\mathbf{n}) &amp;=&amp; \frac{N!}{n_{00}!\ n_{10}!\ n_{11}!\ n_{01}!} 
\left[pq\right]^{n_{11}}\left[p(1-q)\right]^{n_{10}}\left[(1-p)q\right]^{n_{01}}\left[(1-p)(1-q)\right]^{n_{00}} 
\end{eqnarray} 

which, can be simplified to: 

\begin{eqnarray} 
P(\mathbf{n}) &amp;=&amp; \frac{N!}{n_{00}!\ n_{10}!\ n_{11}!\ n_{01}!} 
(p)^{n_{10}+n_{11}}(1-p)^{n_{00}+n_{01}}(q)^{n_{11}+n_{01}}(1-q)^{n_{10}+n_{00}}\\ 
P(\mathbf{n}) &amp;=&amp; \frac{N!}{n_{00}!\ n_{10}!\ n_{11}!\ n_{01}!} 
(p)^{R_1}(1-p)^{R_0}(q)^{C_1}(1-q)^{C_0} 
\end{eqnarray} 


The probability distribution above assumes that there is no relationship 
between $x$ and $y$, and so, if we see a contingency table that is very 
unlikely given the above pdf, we know something is up! But, how sure are we 
that the prior distribution estimates, $p,q$ are correct? Our sample size $N$ 
was very small. That's a troubling question, which can be solved by Laplace 
Smoothing, which sets a uniform prior distribution on $P(x)$ and $P(y)$: 

\begin{eqnarray} 
P(x)=p &amp;=&amp; \frac{R_1+\alpha}{N+\alpha d_x} 
\end{eqnarray} 
where $d_x$ is the number of distinct values $x$ can take on -- in this case 
two. And similarly, for $y$ we'd have the prior: 

\begin{eqnarray} 
P(y)=q &amp;=&amp; \frac{C_1+\alpha}{N+\alpha d_y} 
\end{eqnarray} 

This helps things a little bit, where $\alpha$ is the hyper parameter between 
0 and 1 that controls the ``strength'' of our uniform prior. But one also 
might worry if using a multinomial is even appropriate, given, for very few 
datapoints $N$, the highly discrete nature of our contingency table. 

Fisher's exact test explicitly addresses this discreteness aspect through 
combinatorics. 

Let's recall an experiment where one has a drunken man throw $N$ darts at a 
dartboard with $d$ cells. The number of different ways in which this drunken 
dart player can get $n_1$ darts in the first cell $n_2$ in the second, $n_3$ 
in the third, etc. is: 

\begin{eqnarray} 
W &amp;=&amp; \frac{N!}{n_1!n_2!\cdots n_d!} 
\end{eqnarray} 

Taking the log of this combinatoric factor and applying stirling's 
approximation, we get the Shannon entropy: 

\begin{eqnarray} 
\log W &amp;=&amp; -\sum_{i=1}^d p_i \log p_i \\ 
p_i &amp;=&amp; \frac{n_i}{N} 
\end{eqnarray} 

This is all very interesting because, if one looks at the contingency table 
above, we need only promote our counts $n_{00},n_{01},n_{10},n_{11}$ to a 
compound index: $n_{1},n_2,n_3,n_4$ and we get the same formula: 

\begin{eqnarray} 
W &amp;=&amp; \frac{N!}{n_{00}!n_{01}!n_{10}!n_{11}!} 
\end{eqnarray} 

This is the number of ways one can get a contingency table with the  counts 
$n_{ij}$. But, this is NOT a probability. It is simply a multiplicity count of 
some "state" in phase space, $n_{ij}$. (You'll see above that it's a 
normalization factor for the multinomial). If we want to convert this 
multiplicity count to a probability, we have to be like Kolgomorov and divide 
by the multiplicity of the entire sample space $\Omega$. After all, 

\begin{eqnarray} 
P(x \in X) &amp;=&amp; \frac{\vert X \vert}{\vert \Omega \vert} 
\end{eqnarray} 

Where I'm using bars for ``multiplicity'', or the count of phase space cells 
within some region. 

For our contingency table, above, we can define precisely what the sample 
space $\Omega$ is: all contingency tables with the marginal sums $R_0,R_1$ and 
$C_0,C_1$. This can be written as compound combinatoric factor: 

\begin{eqnarray} 
\vert \Omega \vert &amp;=&amp; \left(\begin{array}{c} 
N \\ C_0 
\end{array}\right) 
\left(\begin{array}{c} 
N \\ R_0 
\end{array}\right)\\ 
&amp;=&amp; \frac{N! N!}{R_0!R_1!C_0!C_1!} 
\end{eqnarray} 

And so we have, doing our division: 

\begin{eqnarray} 
P(n_{00},n_{01},n_{10},n_{11} \vert R_1,R_0,C_1,C_0 ) &amp;=&amp; 
\frac{R_0!R_1!C_0!C_1!}{N! n_{00}!n_{01}!n_{10}!n_{11}!} 
\end{eqnarray} 

Let the joint event $n_{00},n_{01},n_{10},n_{11}$ be specified by 
$\mathbf{n}$. Then we can write 

\begin{eqnarray} 
P(\mathbf{n} \vert \mathbf{R},\mathbf{C} ) &amp;=&amp; 
\frac{R_0!R_1!C_0!C_1!}{N! n_{00}!n_{01}!n_{10}!n_{11}!} 
\end{eqnarray} 

or, more generally, for non-binary categorical variables (check it!): 

\begin{eqnarray} 
P(\mathbf{n} \vert \mathbf{R},\mathbf{C} ) &amp;=&amp; 
\frac{\prod_{i=1}^{d_x}R_i! \prod_{j=1}^{d_y} C_j!}{N! \prod_{i,j} n_{ij}!} 
\end{eqnarray} 

This is a very interesting formula, because it gives the precise, discrete 
probability of seeing some contingency table, conforming to marginal counts 
$\mathbf{R}$ and $\mathbf{C}$. With a little bit of algebra, one will see that 
this combinatoric probability converges to the multinomial we quoted above, by 
noting: 

\begin{eqnarray} 
\lim_{N \to \infty} N! &amp;\approx &amp; \left(\frac{N}{e}\right)^N 
\end{eqnarray} 

and so we get: 

\begin{eqnarray} 
P(n_{00},n_{01},n_{10},n_{11} \vert R_1,R_0,C_1,C_0 ) &amp;\approx &amp; 
\frac{(\frac{R_0}{e})^{R_0}(\frac{R_1}{e})^{R_1}(\frac{C_0}{e})^{C_0}(\frac{C_1}{e})^{C_1}}{(N/e)^N 
(\frac{n_{00}}{e})^{n_{00}}(\frac{n_{01}}{e})^{n_{01}}(\frac{n_{10}}{e})^{n_{10}}(\frac{n_{11}}{e})^{n_{11}}} 
\end{eqnarray} 

All the factors of $e$ cancel out, and we can simplify to get: 

\begin{eqnarray} 
pq &amp;=&amp; \frac{n_{11}}{N}\\ 
p(1-q) &amp;=&amp; \frac{n_{10}}{N}\\ 
(1-p)q &amp;=&amp; \frac{n_{01}}{N}\\ 
(1-p)(1-q) &amp;=&amp; \frac{n_{00}}{N} \\ 
P(\mathbf{n} \vert \mathbf{R},\mathbf{C} ) &amp;=&amp; \frac{N!}{\prod_{i,j} 
n_{ij}! 
}(\frac{R_0}{N})^{R_0}(\frac{R_1}{N})^{R_1}(\frac{C_0}{N})^{C_0}(\frac{C_0}{N})^{C_0} 
\\ 
&amp;=&amp; \frac{N!}{n_{00}!\ n_{10}!\ n_{11}!\ n_{01}!} 
(p)^{n_{10}+n_{11}}(1-p)^{n_{00}+n_{01}}(q)^{n_{11}+n_{01}}(1-q)^{n_{10}+n_{00}} 
\end{eqnarray} 

The same multinomial formula we found above! 

This really isn't so surprising, as it says the combinatoric probability 
converges to the multinomial with fully continuous priors $p,q$ in the large 
sample $N \to \infty$ limit, but it is interesting to note. 

Now, Fisher, when quoting p-values, or significance tests for a relationship 
between $x,y$, would simply use the count of contingency tables that had table 
counts $\mathbf{n}$ more extreme than what's observed. For instance, let's say 
we observe a True/True $x,y$ occurence that is higher than expected under the 
marginals: $n_{11}&gt;N p q$ or $n_{11} &gt; R_1 C_1 / N $: what's the sum of 
the probabilities of tables that have an even \textit{higher} $n_{11}$? This 
is called a one-tailed pvalue significance test, and for the fisher exact test 
and the multinomial method, corresponds to a simple sum. 

I won't get too into the details of implementation now, but suffice to say, 
scipy's got a fisher test calculation all on its own! 



------------------------------------------------------------------------------------------------------------------------ 

Now what hasn't been mentioned is lift. And it relates directly to the laplace 
smoothed priors discussed earlier. Lift is simply: 

\begin{eqnarray} 
l(x\vert y) &amp;=&amp; \frac{P(x \vert y)}{P(x)} = \frac{P(x,y)}{P(x)P(y)} 
\end{eqnarray} 

or, the probability of $x$ taking on some value given $y$ relative to $x$ 
occurring independently. Lift is a number between zero and infinity, and 
basically means: how many more times likely is $x$ going to occur given $y$? 
For low sample size $N &lt; 1000$, it's probably a good idea to smooth the 
priors $P(x),P(y)$ giving us: 

\begin{eqnarray} 
l(x\vert y) &amp;=&amp;  \frac{P(x,y)}{P(x)P(y)} \\ 
&amp;=&amp; \frac{(N+\alpha d_x)(N+\alpha d_y)}{N}\frac{n_{xy}}{n_x n_y } 
\end{eqnarray} 

Where $n_x,n_y$ is the event count of $x$ and $y$. $n_{xy}$ is th joint event 
count of $x,y$. 

------------------------------------------------------------------------------------------------------------------------ 

For those who are heavily implementation-focused, you'll note that this fisher 
test can be done for any type of contingency table -- it doesn't have to be 
two binomial marginal distributions, they could easily be multinomials! -- but 
often the best way to do things is to OneHotEncode the scikit-learn first, and 
then do simple fisher exact tests on each of the pairwise 2x2 contingency 
tables for feature selection. I hope that helps! 

<pre><!--EndFragment--></pre> 
</div>




      <footer class="site-footer">
  <span class="site-footer-owner"><a href="http://localhost:4000">rspeare.github.io</a> is maintained by <a href="">rspeare</a>.</span>
  <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>

  <a href="https://www.linkedin.com/in/rob-speare-aaa6834a">
    <span class="icon  icon--linkedin">
      <svg viewBox="0 0 10000 10000" >
        <path fill="#828282" d="M150.65,100.682c0,27.992-22.508,50.683-50.273,50.683c-27.765,0-50.273-22.691-50.273-50.683
        C50.104,72.691,72.612,50,100.377,50C128.143,50,150.65,72.691,150.65,100.682z M143.294,187.333H58.277V462h85.017V187.333z
        M279.195,187.333h-81.541V462h81.541c0,0,0-101.877,0-144.181c0-38.624,17.779-61.615,51.807-61.615
        c31.268,0,46.289,22.071,46.289,61.615c0,39.545,0,144.181,0,144.181h84.605c0,0,0-100.344,0-173.915
        s-41.689-109.131-99.934-109.131s-82.768,45.369-82.768,45.369V187.333z"/>
      </svg>
    </span>

    <span class="username"></span>
  </a>

</footer>
 

    </section>

  </body>
</html>
