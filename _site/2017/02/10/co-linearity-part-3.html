<!DOCTYPE html>
<html lang="en-us">

  <head>
  <meta charset="UTF-8">
  <title>rspeare.github.io</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/css/cayman.css">
</head>


  <body>
    <section class="page-header">
  <h1 <a href="/" class="project-name">rspeare.github.io </a> </h1> 
  <h2 class="project-tagline"></h2>
  <a href="/" class="btn">Blog</a>  

  <a class="btn" <a href="https://github.com/rspeare"><span class="icon icon--github"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">rspeare</span></a>
 </a>


  <a class="btn" <a href="https://twitter.com/waiting4spark"><span class="icon icon--twitter"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">waiting4spark</span></a>
 </a>

<a href="/about/" class="btn">About</a>  
</section>


    <section class="main-content">
      
      <style TYPE="text/css">
code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
</style>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
    }
});
MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<article class="post" itemscope itemtype="http://schema.org/BlogPosting">

<h2>Co-linearity (Part 3)</h2>
<p class="meta">10 Feb 2017</p>

<div dir="ltr" style="text-align: left;" trbidi="on">As you can imagine, with 
increasing strength of the prior $s$, mentioned above, comes reduction in the 
magnitude of the regression coefficients $\beta$. Instead of using an $L2$ 
norm in the prior, one can also use an L1 norm, and then the log Likelihood 
becomes: 

\begin{eqnarray} 
- \mathcal{L}(\beta \vert X,Y) &amp;=&amp; \frac{(X_{ni} \beta_i - 
Y_n)^2}{2\sigma^2} +  \left(\sum_i \vert \beta_i \vert \right) 
\end{eqnarray} 

Which can be solved as a Quadratic programming problem as long as one puts an 
inequality constraint on the sum of the absolute coefficients of $\beta$: 
$T(\beta) = \sum_i \vert \beta_i \vert &lt; t$. This type of normalization of 
the regression coefficients is called the LASSO, and by the nature of its 
$\beta$ penalization, chooses solutions that are sparse in regressors -- i.e. 
kills off coefficients and features that seem not to matter. With a decreasing 
value of $t$ comes, fewer and few features, as you can imagine, and just like 
our parameter $s$ above, we have explicit control over the ``filtering'' 
pressure of our regression coefficients $\beta$. 

L2 and L1 regularization of regression coefficients lead to slightly different 
solutions. L2 tends to spread coefficient magnitude across clusters of 
variables that are all correlated with the target, while L1 aggressively 
prunes coefficient magnitude to the ``winners'' of the feature set. Making a 
plot of $\beta_i(t), \beta_i(s)$ for both L1 and L2 regularization, reveals 
this. 

The lasso can be very useful when trying to isolate ``what matters'' in a 
regression problem, and just like ridge regression, helps control linear 
dependence and colinearity within the data matrix, but one can also use simple 
clustering techniques to choose the ``best'' set of features. For example, 
take the normalized correlation matrix: 

\begin{eqnarray} 
\tilde{X}_{ni} &amp;=&amp; \frac{X_{ni}-\mu_i}{\sigma_i} \\ 
\rho_{ij} &amp;=&amp; \mathrm{corr}(x_i,x_j) = \frac{1}{N} 
\tilde{X}_{ni}\tilde{X}_{nj} 
\end{eqnarray} 

The upper diagonal portion of $\rho_{ij}$ represents a graph, where the nodes 
are the features $i$ and the edges are the matrix entries -- each between 0 
and 1. We can ``cluster'' our set of features very easily, by simply 
thresholding $\rho_{ij} &gt; \epsilon$ and then picking out connected 
components from the matrix. The connected components -- depending upon how 
many of them there are, each represent ``feature groups'', from which one can 
choose the most highly correlated feature with the target: 

\begin{eqnarray} 
\lbrace C_n \rbrace &amp;=&amp; \mathrm{conn}(\rho_{ij} &gt; \epsilon) \\ 
x_\mathrm{c} &amp;=&amp; \mathrm{max}_{i \in c} \left(\mathrm{corr}(x_i, 
y)\right)  \ \ \forall c \in \lbrace C_n \rbrace 
\end{eqnarray} 

Obviously, this filtered set of features -- and their multiplicity -- will be 
a function of $\epsilon$. As $\epsilon \to 1$ we will have all features come 
out, $x_c = x_i$, and as $\epsilon \to 0$ we will have the single, most 
highly-correlated feature: $x_c = \max_i \mathrm{corr}(x_i,y)$. 

The whole point of doing this, of course, is to find a set of features $x_c$ 
that are statistically de-coupled from one - another, and it really reduces to 
a supervised down-sampling of the initial data. 

Because regularization paths are so popular, especially from a diagnostic 
point of view, it's worth mentioning that one of my favorite algorithms for 
sequentially adding in features to a regression problem is LARS -- or least 
Angle Regression. The basic idea comes from boosting, but I'll get to it in 
the next post. 
</div>




      <footer class="site-footer">
  <span class="site-footer-owner"><a href="http://localhost:4000">rspeare.github.io</a> is maintained by <a href="">rspeare</a>.</span>
  <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>

  <a href="https://www.linkedin.com/in/rob-speare-aaa6834a">
    <span class="icon  icon--linkedin">
      <svg viewBox="0 0 10000 10000" >
        <path fill="#828282" d="M150.65,100.682c0,27.992-22.508,50.683-50.273,50.683c-27.765,0-50.273-22.691-50.273-50.683
        C50.104,72.691,72.612,50,100.377,50C128.143,50,150.65,72.691,150.65,100.682z M143.294,187.333H58.277V462h85.017V187.333z
        M279.195,187.333h-81.541V462h81.541c0,0,0-101.877,0-144.181c0-38.624,17.779-61.615,51.807-61.615
        c31.268,0,46.289,22.071,46.289,61.615c0,39.545,0,144.181,0,144.181h84.605c0,0,0-100.344,0-173.915
        s-41.689-109.131-99.934-109.131s-82.768,45.369-82.768,45.369V187.333z"/>
      </svg>
    </span>

    <span class="username"></span>
  </a>

</footer>
 

    </section>

  </body>
</html>
