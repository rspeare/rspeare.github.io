<!DOCTYPE html>
<html lang="en-us">

  <head>
  <meta charset="UTF-8">
  <title>rspeare.github.io</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/css/cayman.css">
</head>


  <body>
    <section class="page-header">
  <h1 <a href="/" class="project-name">rspeare.github.io </a> </h1> 
  <h2 class="project-tagline"></h2>
  <a href="/" class="btn">Blog</a>  

  <a class="btn" <a href="https://github.com/rspeare"><span class="icon icon--github"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">rspeare</span></a>
 </a>


  <a class="btn" <a href="https://twitter.com/waiting4spark"><span class="icon icon--twitter"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">waiting4spark</span></a>
 </a>

<a href="/about/" class="btn">About</a>  
</section>


    <section class="main-content">
      
      <style TYPE="text/css">
code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
</style>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
    }
});
MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<article class="post" itemscope itemtype="http://schema.org/BlogPosting">

<h2>Least Chi-Squared -- or Maximum Log-Likelihood -- Estimation of Parameters</h2>
<p class="meta">15 Apr 2014</p>

<div dir="ltr" style="text-align: left;" trbidi="on">Least Chi-Squared -- or 
Maximum Log-Likelihood -- Estimation of Parameters 

Let's say your given some data, a list of x and y values, which seem to be 
related by a model. One example is a perfect linear relation ship with some 
scatter, where 

\begin{eqnarray} 
y_i \approx m*x_i+b 
\end{eqnarray} 

Where $m$ is a best fit slope and $b$ a vertical offset. Since this is not a 
perfect relationship, we must introduce an error term into our model: 

\begin{eqnarray} 
y_i &amp;=&amp; m*x_i+b + e_i\\ 
y_i &amp;=&amp; f(x_i) + e_i \\ 
f(x) &amp;=&amp; mx+b 
\end{eqnarray} 

$f(x_i)$ in this instance, represents our model, or, our "perceived" 
relationship between $x$ and $y$ values. Another example is when the $y_i$ 
values are just centered around some single value -- i.e. a random variable. 
Then we have: 

\begin{eqnarray} 
y_i &amp;=&amp; f(x_i) + e_i \\ 
f(x) &amp;=&amp; \mu 
\end{eqnarray} 

For a random variable $Y$ centered about zero, we might say $f(x)=0$. This is 
STILL a model! 

But the point being, if we want to make a "best-guess" at the parameters 
within our model -- such as $m$ and $b$ above, or $\mu$ -- we are going to 
have to come up with some way of describing the error terms, $e_i$. 

A reasonable assumption is that those error terms are drawn from some 
Probability density; let's say a Gaussian. 

\begin{eqnarray} 
e_i &amp;\sim &amp; \frac{1}{\sqrt{2\pi \sigma^2}}e^{-\frac{e_i^2}{2\sigma^2}} 
\end{eqnarray} 

We can isolate the error term in the following way: we subtract our "output" 
value $y_i$ from the model $f(x_i)$, this results in, for a random variable 
centered about a true value $\mu$, $f(x)=\mu$: 

\begin{eqnarray} 
e_i &amp;=&amp; y_i - f(x_i) \\ 
P(y_i \vert \mu, \sigma^2) &amp;=&amp; \frac{1}{\sqrt{2\pi 
\sigma^2}}e^{-\frac{\left(y_i - f(x_i)\right)^2}{2\sigma^2}} 
\end{eqnarray} 

Now, this is the probability of a single data point-pair, $x_i,y_i$, assuming 
our model $f(x)$, and an error term that is described by this Gaussian 
distribution. If we were to count up all $N$ data point pairs, and calculate 
the probability of observing the whole set $D:{x_i}$, we would take the 
product of the individual probabilities: 

\begin{eqnarray} 
P(D| \mu, \sigma^2) &amp;=&amp; \Pi_i P(y_i \vert \mu, \sigma^2)\\ 
&amp;=&amp; (2\pi \sigma^2)^{-\frac{N}{2}} e^{-\sum_i \frac{\left(y_i - 
f(x_i)\right)^2}{2\sigma^2}} 
\end{eqnarray} 

Notice that this conditional probability has the same structure as our 
$P(D\vert H_i) $ term from Bayes Theorem, before. Our "hypothesis" in this 
case, is the mean and variance; or, the parameters of our model. We can 
construct posterior probability densities on those parameters in the usual 
way: 

\begin{eqnarray} 
P(H_i \vert D) &amp;=&amp; \frac{P(D \vert H_i)P(H_i)}{P(D)}\\ 
&amp;=&amp; \frac{P(D \vert H_i)P(H_i)}{\sum_i P(D \vert H_i)P(H_i)}\\ 
P(\mu, \sigma^2 \vert D) &amp;=&amp; \frac{P(D \vert \mu, \sigma^2)P(\mu, 
\sigma^2)}{\int \int P(D \vert \mu, \sigma^2)P(\mu, \sigma^2)d\mu d\sigma^2} 
\end{eqnarray} 

The denominator in these terms is simply a normalizing factor. If one wants to 
construct the posterior on a single parameter, say $\mu$, we merely perform 
what's called marginalization: 

\begin{eqnarray} 
P(\mu \vert D) &amp;=&amp; \int P(\mu, \sigma^2 \vert D) d\sigma^2 
\end{eqnarray} 

This equation reads "the probability of some mean $\mu$, given the data $D$, 
an array of x and y values". 

Ok, so why do we care? Well, in Bayes Theorem above, we implicitly used 
something called a Likelihood function, which is the Likelihood of seeing the 
data, given some model and its parameters: 
\begin{eqnarray} 
\mathcal{L}(D \vert \mu, \sigma^2)=P(D \vert \mu, \sigma^2) &amp;=&amp; (2\pi 
\sigma^2)^{-\frac{N}{2}} e^{-\sum_i \frac{\left(y_i - 
f(x_i)\right)^2}{2\sigma^2}} 
\end{eqnarray} 

Now the term in our exponential above can be simplified. Defining the $\chi^2$ 
statistic for our data: 

\begin{eqnarray} 
\chi^2 &amp;=&amp; \sum_i \frac{\left(y_i - f(x_i)\right)^2}{\sigma^2}\\ 
\mathcal{L}(D \vert \mu, \sigma^2)&amp;=&amp; (2\pi \sigma^2)^{-\frac{N}{2}} 
e^{-\frac{\chi^2}{2}} 
\end{eqnarray} 

Pretty cool right? So, maximizing Likelihood is minimizing $\chi^2$. We can 
take the log of this Likelihood function to write things in a more 
illuminating fashion: 

\begin{eqnarray} 
\log\left[\mathcal{L}(D \vert \mu, \sigma^2)\right]&amp;=&amp; 
-\frac{N}{2}\log (2\pi \sigma^2)-\frac{\chi^2}{2} 
\end{eqnarray} 

Taking the derivative with respect to $\mu$ and setting equal to zero yields 
are critical value of $\mu$ -- let's call it $\hat{\mu}$ -- that yields 
maximum Likelihood: 

\begin{eqnarray} 
\frac{\partial}{\partial \mu}\log\left[\mathcal{L}(D \vert \mu, 
\sigma^2)\right] &amp;=&amp;-\frac{\partial}{\partial \mu}\chi^2/2\\ 
0&amp;=&amp;\frac{\sum_i x_i - N\hat{\mu}}{\sigma^2}\\ 
\hat{\mu} &amp;=&amp; \sum_i \frac{x_i}{N} 
\end{eqnarray} 

our standard "average". This is our estimator of the mean. 

Now for an estimator of the variance -- or, in our older terms, the value of 
$\sigma$ for which Likelihood is maximized: 

\begin{eqnarray} 
\frac{\partial}{\partial \sigma}\log\left[\mathcal{L}(D \vert \mu, 
\sigma^2)\right] &amp;=&amp; 0\\ 
\frac{N}{\hat{\sigma}}&amp;=&amp;\frac{\sum_i (x_i-\mu)^2}{\hat{\sigma}^3}\\ 
\hat{\sigma^2} &amp;=&amp;\frac{\sum_i (x_i-\mu)^2}{N} 
\end{eqnarray} 

Interesting, our standard sum of squares. Notice however, that we've used our 
"true" mean $\mu$ in the above equation, not our estimator $\hat{\mu}$. In 
order to account for this, one can write 

\begin{eqnarray} 
\hat{\sigma^2} &amp;=&amp;\frac{\sum_i (x_i-\mu)^2}{N}\\ 
N\hat{\sigma^2} &amp;=&amp;\sum_i (x_i-\mu)^2\\ 
&amp;=&amp;\sum_i (x_i-\hat{\mu})^2+N(\hat{\mu}-\mu)^2\\ 
&amp;=&amp; Ns^2+N(\sum_i \frac{x_i}{N}-\mu)^2\\ 
N\hat{\sigma^2} &amp;=&amp; Ns^2+N(\sum_i \frac{(x_i-\mu)^2}{N^2}\\ 
N\hat{\sigma^2} &amp;=&amp; Ns^2+\hat{\sigma^2}\\ 
\frac{(N-1)}{N}\hat{\sigma^2} &amp;=&amp; s^2 
\end{eqnarray} 

Where we now have a new estimator for the variance, which requires no "true 
information": 
\begin{eqnarray} 
s^2&amp;=&amp; \frac{\sum_i (x_i-\hat{\mu})^2}{N} 
\end{eqnarray} 

If we want to best emulate the "true information estimator", we had best 
divide by $N-1$ instead of $N$. Another way to show this is that $s^2$ is a 
biased estimator: it's expectation value, or average over an infinite number 
of data sets, is not $\sigma^2$ but in fact: 

\begin{eqnarray} 
\langle s^2 \rangle &amp;=&amp; \frac{N}{N-1}\sigma^2 
\end{eqnarray} 

To review, let us write our Posterior distribution one more time: 

\begin{eqnarray} 
P(H_i \vert D) &amp;=&amp; \frac{P(D \vert H_i)P(H_i)}{\sum_i P(D \vert 
H_i)P(H_i)}\\ 
P(\mu, \sigma^2 \vert D) &amp;=&amp; \frac{\mathcal{L}(D \vert \mu, 
\sigma^2)P(\mu, \sigma^2)}{\int \int P(D \vert \mu, \sigma^2)P(\mu, 
\sigma^2)d\mu d\sigma^2}\\ 
P(\mu, \sigma^2 \vert D) &amp;=&amp; \frac{(2\pi \sigma^2)^{-\frac{N}{2}} 
e^{-\frac{\chi^2}{2}}P(\mu, \sigma^2)}{\int \int P(D \vert \mu, 
\sigma^2)P(\mu, \sigma^2)d\mu d\sigma^2} 
\end{eqnarray} 

The factor $P(\mu,\sigma^2)$ on the right hand side is called a prior, and 
determines our a priori information about the parameters $\mu$ and $\sigma^2$. 
What's nice about Bayes' construction is that the posterior from one data set 
can be used as the prior for another -- $D$ just goes into more initial 
information. (Perhaps we should've written the prior 

\begin{eqnarray} 
P(\mu,\sigma^2 \vert \mathrm{old\ dataset}) 
\end{eqnarray} 
</div>




      <footer class="site-footer">
  <span class="site-footer-owner"><a href="http://localhost:4000">rspeare.github.io</a> is maintained by <a href="">rspeare</a>.</span>
  <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>

  <a href="https://www.linkedin.com/in/rob-speare-aaa6834a">
    <span class="icon  icon--linkedin">
      <svg viewBox="0 0 10000 10000" >
        <path fill="#828282" d="M150.65,100.682c0,27.992-22.508,50.683-50.273,50.683c-27.765,0-50.273-22.691-50.273-50.683
        C50.104,72.691,72.612,50,100.377,50C128.143,50,150.65,72.691,150.65,100.682z M143.294,187.333H58.277V462h85.017V187.333z
        M279.195,187.333h-81.541V462h81.541c0,0,0-101.877,0-144.181c0-38.624,17.779-61.615,51.807-61.615
        c31.268,0,46.289,22.071,46.289,61.615c0,39.545,0,144.181,0,144.181h84.605c0,0,0-100.344,0-173.915
        s-41.689-109.131-99.934-109.131s-82.768,45.369-82.768,45.369V187.333z"/>
      </svg>
    </span>

    <span class="username"></span>
  </a>

</footer>
 

    </section>

  </body>
</html>
