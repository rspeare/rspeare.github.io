<!DOCTYPE html>
<html lang="en-us">

  <head>
  <meta charset="UTF-8">
  <title>rspeare.github.io</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/css/cayman.css">
</head>

  <body>
    <section class="page-header">
  <h1 <a href="/" class="project-name">rspeare.github.io </a> </h1> 
  <h2 class="project-tagline"></h2>
  <a href="/" class="btn">Blog</a>  

  <a class="btn" <a href="https://github.com/rspeare"><span class="icon icon--github"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">rspeare</span></a>
 </a>


  <a class="btn" <a href="https://twitter.com/waiting4spark"><span class="icon icon--twitter"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">waiting4spark</span></a>
 </a>

<a href="/about/" class="btn">About</a>  
</section>


    <section class="main-content">
      
      <style TYPE="text/css">
code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
</style>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
    }
});
MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<h2>Bivariate Normal Distribution</h2>
<p class="meta">22 Feb 2014</p>

<div dir="ltr" style="text-align: left;" trbidi="on">Because I have had a 
great deal of trouble finding information on the Bivariate Normal distribution 
-- or at least, information that is easy for me to understand -- I'd like to 
take a moment to write down my understanding of the concept, which stems 
mainly from the Characteristic function. 

Working with results from last time, on the central limit theorem, we see that 
we can describe a Probability density function by it's fourier transform, the 
characteristic function 

\begin{eqnarray} 
P(\mathbf{x}) &amp;=&amp; \frac{1}{(2\pi)^{d/2}}\int d^dk e^{i\mathbf{k}\cdot 
\mathbf{x}-i\mathbf{k}\cdot \mathbf{c_1}-\mathbf{k}\cdot 
\mathbf{c_2}\cdot\mathbf{k}+\dots } 
\end{eqnarray} 

Where, for now, I will ignore higher order terms corresponding to 
non-gaussianity. If we zero about the mean -- absorbing our first cumulant, 
$\mathbf{c_1}$ into $\mathbf{x}$ -- we find that we have a simple fourier 
transform of a Gaussian, which is itself a Gaussian. 

\begin{eqnarray} 
P(\mathbf{x}) &amp;=&amp; \frac{1}{(2\pi)^{d/2}}\int d^dk e^{i\mathbf{k}\cdot 
\mathbf{x}-\mathbf{k}\cdot \mathbf{c_2}\cdot\mathbf{k}}\\ 
P(\mathbf{x}) &amp;=&amp; 
\frac{1}{(2\pi)}\frac{1}{|\mathbf{c_2}|}e^{\mathbf{x}\cdot 
\mathbf{c_2^{-1}}\cdot \mathbf{x}} 
\end{eqnarray} 

Where $\mathbf{c_2}$ is the covariance matrix -- rank (0,2) tensor -- in $k$ 
space. 

If we are working with two random variables, $x_1,x_2$, we see that our $k$ 
integration takes place over two dimensions, so we are left with 
\begin{eqnarray} 
P(x_1,x_2) &amp;=&amp; \frac{1}{2\pi}\int dk_1 dk_2 e^{i(k_1 x_1+k_2 
x_2)}e^{-\frac{1}{2}\left( c_{11}k_1^2 +2c_{12}k_1k_2 +c_{22}k_2^2 \right)} 
\end{eqnarray} 

Notice that our characteristic function has a simple form, which we can now 
play with in order to construct estimators for the correlation coefficient 
$\rho$ commonly defined as 

\begin{eqnarray} 
\rho &amp;=&amp; \frac{c_{12}}{\sigma_1 \sigma_2} 
\end{eqnarray} 

Let's take a look. First note that $c_{11}=\sigma_1^2$ and 
$c_{22}=\sigma_2^2$. These are the second cumulants of $x_1$ and $x_2$ 
respectively, their variance with respect to self. Re-writing our bivariate 
normal with these definitions, and assuming **zero mean**, (or $c_1=c_2=0$), 
we find 

\begin{eqnarray} 
\mathbf{c_2^{-1}}&amp;=&amp; \left(\begin{array}{cc} 
 \frac{1}{\sigma_1^2} &amp; \frac{\rho}{\sigma_1 \sigma_2} \\ 
 \frac{\rho}{\sigma_1 \sigma_2} &amp; \frac{1}{\sigma_2^2} \\ 
\end{array}\right)\\ 
\vert \mathbf{c_2} \vert &amp;=&amp; \sigma_1^2 \sigma_2^2 (1-\rho^2) 
\end{eqnarray} 

\begin{eqnarray} 
P(x_1,x_2) &amp;=&amp; \frac{1}{2\pi}\frac{1}{\sigma_1 \sigma_2 
\sqrt{1-\rho^2}}^{-\frac{1}{2}\left( \frac{x_1^2}{\sigma_1^2} +2\rho 
\frac{x_1x_2}{\sigma_1 \sigma_2} +\frac{x_2^2}{\sigma_2^2} \right)} 
\end{eqnarray} 

This last, nasty equation is the typical bivariate normal seen in the 
literahchurr. (literature in snoot-spell). But how the heck to get an 
estimator for the covariance?!?! We know that 

\begin{eqnarray} 
\sigma_1^2 &amp;=&amp; \langle x_1^2 \rangle-\langle x_1 \rangle^2 = c_{11} \\ 
&amp;=&amp; -\frac{\partial^2 P}{\partial 
k_1^2}\vert_{(k_1,k_2)=(0,0)}+\left(\frac{\partial P}{\partial 
k_1}\vert_{(k_1,k_2)=(0,0)}\right)^2 
\end{eqnarray} 

and 

\begin{eqnarray} 
\sigma_2^2 &amp;=&amp; \langle x_2^2 \rangle-\langle x_2 \rangle^2 = c_{22} \\ 
&amp;=&amp; -\frac{\partial^2 P}{\partial 
k_2^2}\vert_{(k_1,k_2)=(0,0)}+\left(\frac{\partial P}{\partial 
k_2}\vert_{(k_1,k_2)=(0,0)}\right)^2 
\end{eqnarray} 

The second term in both expansions -- the first derivative of P(k) squared -- 
is unnecessary, since we are assuming zero mean. 

Let us show this trick of differentiating the characteristic function in order 
to get cumulant estimators explicitly, and then apply it to the covariance 
$\rho$ 

\begin{eqnarray} 
P(k_1,k_2) &amp;=&amp; \frac{1}{2\pi} \int dx_1 dx_2 e^{-i(k_1 x_1 + k_2 
x_2}P(x_1,x_2) = e^{-\frac{1}{2}\left( c_{11}k_1^2 + 2 c_{12}k_1 k_2 + c_{22} 
k_2^2 \right)} 
\end{eqnarray} 

Differentiating with respect to $k_1$ twice 

\begin{eqnarray} 
\frac{\partial P}{\partial k_1} &amp;=&amp; \frac{1}{2\pi} \int dx_1 dx_2 
-ix_1 e^{-i(k_1 x_1 + k_2 x_2}P(x_1,x_2) = \left( c_{11}k_1+ c_{12}k_2 \right) 
e^{-\frac{1}{2}\left( c_{11}k_1^2 + 2 c_{12}k_1 k_2 + c_{22} k_2^2 \right)}\\ 
\frac{\partial^2 P}{\partial k_1^2}&amp;=&amp; \frac{1}{2\pi} \int dx_1 dx_2 
-x_1^2 e^{-i(k_1 x_1 + k_2 x_2}P(x_1,x_2) = \left( c_{11}\right) 
e^{-\frac{1}{2}\left( c_{11}k_1^2 + 2 c_{12}k_1 k_2 + c_{22} k_2^2 \right)} 
\end{eqnarray} 

and setting $k_1=k_2=0$, we find 

\begin{eqnarray} 
\frac{\partial^2 P}{\partial k_1^2}\vert_{(k_1,k_2)=(0,0)}&amp;=&amp; 
\frac{1}{2\pi} \int dx_1 dx_2 -x_1^2 P(x_1,x_2) = \left( c_{11}\right) \\ 
c_{11} &amp;=&amp; \langle x_1^2 \rangle 
\end{eqnarray} 

Now for the covariance, we can take the derivative with respect to $k_1$, then 
$k_2$, to get, 

\begin{eqnarray} 
\frac{\partial P}{\partial k_1} &amp;=&amp; \frac{1}{2\pi} \int dx_1 dx_2 
-ix_1 e^{-i(k_1 x_1 + k_2 x_2}P(x_1,x_2) = \left( c_{11}k_1+ c_{12}k_2 \right) 
e^{-\frac{1}{2}\left( c_{11}k_1^2 + 2 c_{12}k_1 k_2 + c_{22} k_2^2 \right)}\\ 
\frac{\partial^2 P}{\partial k_1 \partial k_2} &amp;=&amp; \frac{1}{2\pi} \int 
dx_1 dx_2 -x_1x_2 e^{-i(k_1 x_1 + k_2 x_2}P(x_1,x_2) = \left( c_{12} \right) 
e^{-\frac{1}{2}\left( c_{11}k_1^2 + 2 c_{12}k_1 k_2 + c_{22} k_2^2 \right)}\\ 
\frac{\partial^2 P}{\partial k_1 \partial k_2}\vert_{(k_1,k_2)=(0,0)} 
&amp;=&amp; \frac{1}{2\pi} \int dx_1 dx_2 -x_1x_2 P(x_1,x_2) = c_{12} \\ 
c_{12} &amp;=&amp; \langle x_1 x_2 \rangle \\ 
\rho &amp;=&amp; \frac{c_{12}}{\sigma_1 \sigma_2}\\ 
\rho &amp;=&amp; \frac{\langle x_1 x_2 \rangle}{\left(\langle x_1^2 \rangle 
\langle x_2^2 \rangle\right)^{1/2}}\\ 
\end{eqnarray} 

And voila! We have found the estimator -- without taking into account bias -- 
of the correlation coefficient! 
</div>




      <footer class="site-footer">
  <span class="site-footer-owner"><a href="http://localhost:4000">rspeare.github.io</a> is maintained by <a href="">rspeare</a>.</span>
  <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>

  <a href="https://www.linkedin.com/in/rob-speare-aaa6834a">
    <span class="icon  icon--linkedin">
      <svg viewBox="0 0 10000 10000" >
        <path fill="#828282" d="M150.65,100.682c0,27.992-22.508,50.683-50.273,50.683c-27.765,0-50.273-22.691-50.273-50.683
        C50.104,72.691,72.612,50,100.377,50C128.143,50,150.65,72.691,150.65,100.682z M143.294,187.333H58.277V462h85.017V187.333z
        M279.195,187.333h-81.541V462h81.541c0,0,0-101.877,0-144.181c0-38.624,17.779-61.615,51.807-61.615
        c31.268,0,46.289,22.071,46.289,61.615c0,39.545,0,144.181,0,144.181h84.605c0,0,0-100.344,0-173.915
        s-41.689-109.131-99.934-109.131s-82.768,45.369-82.768,45.369V187.333z"/>
      </svg>
    </span>

    <span class="username"></span>
  </a>

</footer>
 

    </section>

  </body>
</html>
